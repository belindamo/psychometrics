{"id":"paper_1702500000000","title":"Attention Is All You Need","authors":"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","journal":"NeurIPS","year":"2017","doi":"10.5555/3295222.3295349","url":"https://arxiv.org/abs/1706.03762","keyAssumptions":"RNNs and CNNs are necessary for sequence transduction; Sequential computation limits parallelization","citation":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.","notes":"Foundational paper introducing the Transformer architecture. Key innovation: self-attention mechanism allows modeling dependencies without recurrence or convolution.","addedDate":"2024-01-15T10:00:00Z"}